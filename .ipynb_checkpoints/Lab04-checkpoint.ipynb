{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d929ab0a-6b86-40d3-86cf-3aed4e31d287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark: Spark's API for Python.\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef95e22-4437-47af-b8c0-e81872589972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc9436-220a-4f53-af92-ee7af709e8a6",
   "metadata": {},
   "source": [
    "### What is SparkSession?\n",
    "Spark Session: A unified entry point for DataFrame and Dataset APIs.It's object \"spark\" is default available in spark-shell and it can be created programmatically using SparkSession builder pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca1398-cb6d-41d0-a505-bf5caccd2abc",
   "metadata": {},
   "source": [
    "In Apache Spark 2.x and later, the `SparkSession` is the entry point to any Spark functionality. When you want to run a Spark application, you first need to create a SparkSession. \n",
    "\n",
    "The `SparkSession.builder().getOrCreate()` method is a way to ensure that a SparkSession is created only once in an application.\n",
    "\n",
    "`SparkSession.builder()`: This returns a SparkSession.Builder object, which is a builder for a SparkSession. With the builder, you can configure options for the SparkSession, such as appName, master, and various Spark configurations using the config method.\n",
    "\n",
    "`getOrCreate()`: When called on a SparkSession.Builder object, this method: Retrieves the existing SparkSession if one already exists.\n",
    "Creates a new SparkSession if none exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f96269-d77e-446c-88c4-300265be9f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Spark session\n",
    "spark = SparkSession.builder.getOrCreate() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd72f6d-8785-4fc5-b0ab-8e39693b5031",
   "metadata": {},
   "source": [
    "### Create a DataFrame from a CSV file\n",
    "`read`:\n",
    "This is a method associated with SparkSession and it returns a DataFrameReader that can be used to read data. The read method provides functionality to read data from various sources into a Spark DataFrame.\n",
    "\n",
    "`option('header', 'true')`:\n",
    "The option method allows you to specify options when reading data. In this case, the option being set is 'header' with the value 'true'. This means that the first row of the CSV file (Traffic_Crashes_-_Crashes.csv) is considered as a header and will be used to name the columns of the DataFrame.\n",
    "\n",
    "If this option wasn't set (or set to 'false'), the CSV file would be read without considering the first row as a header, and default column names would be assigned (like _c0, _c1, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6428b5f-9576-406e-962b-288d6e2a2305",
   "metadata": {},
   "source": [
    "**Q1: Load the data from the csv files into DataFrames.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f68193-cf04-405e-a419-e4a3a7c35655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the csv files into DataFrames.\n",
    "crashes = spark.read.option('header', 'true').csv('Traffic_Crashes_-_Crashes.csv')\n",
    "vehicles = spark.read.option('header' , 'true').csv('Traffic_Crashes_-_Vehicles.csv')\n",
    "peoples = spark.read.option( 'header', 'true').csv('Traffic_Crashes_-_People.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e8ca07-e4e2-488f-864d-b0bcec7fcf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what is the type of crashes\n",
    "print(type(crashes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e74fe0-694c-4788-b431-7bd5db049aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see what is the data type of each DataFrame \n",
    "crashes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905166e6-9e3a-4e3a-a1b9-c8b3631d80d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443378c-453e-40a0-9169-8ecb0aafa8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "peoples.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eee590-a291-4c4f-981d-fbbf0bab11ea",
   "metadata": {},
   "source": [
    "In PySpark, the `pyspark.sql.types` module provides a collection of data types that you can use to specify the schema of a DataFrame. When you're working with data in Spark, sometimes you might need to explicitly define or cast data to a specific type. This is where these imports come into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07edce4-7663-4026-897a-ad04c33d0365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql. types import StringType \n",
    "from pyspark.sql. types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada2241f-cefe-4c49-97b7-4615163449d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a299587-c686-433c-8caa-01f7bae3c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810afc32-e74f-45a0-98d7-bb1b7786c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118819c5-60ca-477f-a060-98ba4f2bc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc56574f-3b30-4122-ab77-2626a6296ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986f738-8973-436e-bc9b-fde5b14f2974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516d5c8-eedd-470d-9c1c-0917a5af7e65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
